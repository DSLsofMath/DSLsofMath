* DSLsofMath Week 7: Linear Algebra and the Laplace transform
+ Mix of Chapter 7 and Chapter 8 (to get started early on the Laplace
  transform).
** Lecture 7.1: Linear Algebra 1
+ The DSL of the this lecture describes
  vectors, linear transforms, matrices
*** L7.1a [[file:DSLsofMath L7.1.pdf]]
+ class (Field s, AddGroup v) => VectorSpace v s where
    scale :: s -> v -> v  -- s for "scalars", v for "vectors"
+ Laws: {- very compact: add types, expand -}
  ∀ c. GroupHom(scale c, (addV,zeroV,negateV), (addV,zeroV,negateV))
       GroupHom(scale,   (addS,zeroS,negateS), (addF,zeroF,negateF))
      MonoidHom(scale,   (mulS,oneS)         , ((.), id))
+ Laws expanded one step + with types
  scale c :: v->v;
  addV :: v->v->v; zeroV :: v; negateV :: v->v
  ∀ c. H2(scale c, addV,    addV   )  &&
       H0(scale c, zeroV,   zeroV  )  &&
       H1(scale c, negateV, negateV)

  scale :: s -> (v->v)
  addS :: s->s->s;                zeroS :: s;      negateS :: s->s
  addF :: (v->v)->(v->v)->(v->v); zeroF :: (v->v); negateF :: (v->v)->(v->v)
         H2(scale, addS,    addF   ) &&
	 H0(scale, zeroS,   zeroF  ) &&
	 H1(scale, negateS, negateF)

  scale :: s -> (v->v)
  mulS :: s->s->s;                oneS :: s
  (.)  :: (v->v)->(v->v)->(v->v); id :: (v->v)
       H2(scale, mulS, (.)) &&
       H0(scale, oneS, id)
+ Laws expanded two steps: (types unchanged)
  ∀ c. (∀ v₁, v₂. scale c (addV v₁ v₂) = addV (scale c v₁) (scale c v₂)) &&
       (scale c zeroV = zeroV) &&
       (∀ v. scale c (negateV v) = negateV (scale c v))

       (∀ s₁, s₂. scale (addS s₁ s₂) = addF (scale s₁) (scale s₂)) &&
	 (scale zeroS = zeroF) &&
	 (∀ s. scale (negateS s) = negateF (scale s))

       (∀ s₁, s₂. scale (mulS s₁ s₂) = scale s₁ . scale s₂) &&
       (scale oneS = id)
+ Laws expanded three steps: (first group unchanged)
  ∀ c. (∀ v₁, v₂. scale c (addV v₁ v₂) = addV (scale c v₁) (scale c v₂)) &&
       (scale c zeroV = zeroV) &&
	 (∀ v. scale c (negateV v) = negateV (scale c v))

       (∀ s₁, s₂, v. scale (addS s₁ s₂) v = addV (scale s₁ v) (scale s₂ v)) &&
	 (∀ v. scale zeroS v = zeroV) &&
	 (∀ s, v. scale (negateS s) v = negateV (scale s v))

       (∀ s₁, s₂, v. scale (mulS s₁ s₂) v = scale s₁ (scale s₂ v)) &&
       (∀ v. scale oneS v = v)
+ Laws with (overloaded) operators, 0, and 1:
    ∀ c. (∀ v₁, v₂. c ◃ (v₁+v₂) = (c◃v₁) + (c◃v₂)) &&
         (c◃0 = 0) &&
	 (∀ v. c◃(-v) = -(c◃v))

    ∀ v. (∀ s₁, s₂. (s₁+s₂)◃v = (s₁◃v)+(s₂◃v)) &&
	 (0◃v = 0) &&
	 (∀ s. (-s)◃v = -(s◃v))

    ∀ v. (∀ s₁, s₂. (s₁·s₂)◃v = s₁◃(s₂◃v)) &&
         (1◃v = v)
+ instance Field s => VectorSpace s s where scale = (*)
  + here |s = v|, so |scale : s->v->v| becomes |scale : s->s->s|
  + we can see the scalars as a "1-dimensional vector space" with canonical basis one
    + scale s one = s*one = s
+ A two-dimensional example: file:XYPlaneExampleVectors.png
  data Axes   = X | Y
  type XYVec  = Axes -> REAL
  + Example vectors:
    v₁ X = 4   ; v₁ Y = 1   ;
    v₂ X =   1 ; v₂ Y =   3 ;
    v₃ X = 4+1 ; v₃ Y = 1+3 ;
  + instance VectorSpace XYVec REAL where scale = scaleF
    scaleF :: REAL -> (Axes -> REAL) -> (Axes -> REAL)
    scaleF c v = \a -> c * v a
+ Linear Combinations
  -- Math notation: Σᵢ aᵢ◃vᵢ = Σᵢ (scale aᵢ vᵢ)
  linComb :: (Finite g, VectorSpace v s) => (g->s) -> (g->v) -> v
  linComb a v = sum  (map  (\i->scale (a i) (v i))
                           finiteDomain
	             )
+ Linearly independent, span, and basis:
  + A collection of vectors v₀, ..., vₙ is *linearly independent* iff
    ∀ a:{0..n}->s. (linComb a v == 0) ⇔ (∀ i. aᵢ==0)
  + The span S of a collection of vectors V (over a field S) is the
    set of all linear combinations of those vectors.
  + A basis B for a vector space V over a field S is a linearly
    independent collection of vectors which spans V.
+ Example (canonical) basis: for
  + let G = {0..n}; V = G->s; e : G -> V;  e : G -> (G -> s)
        eᵢ j = if i==j then 1 else 0    --      i     j
  + Then e is a basis for V
+ Next up: linear transformation (homomorphism between vector spaces)
*** L7.1b [[file:DSLsofMath L7.1b.pdf]]
+ h : U -> V   where U and V and vector spaces over the same scalars s
+ LinTran(h,U,V) =    H₀(h,zeroᵤ,zeroᵥ)        --   h zeroᵤ == zeroᵥ
                 ∧   H₂(h,(+ᵤ),(+ᵥ))          -- ∀ x, y. h (x +ᵤ y) == (h x) +ᵥ (h y)
		 ∧∀c.H₁(h,scaleᵤ c,scaleᵥ c)  -- ∀ c, x. h (scaleᵤ c x) = scaleᵥ c (h x)
+ Examples:   ∀c. LinTran(scaleᵥ c, V, V)
              ∀i. LinTran(apply i, g->s, s)  -- "projections"
	      apply i v = v i
+ Linear transformations between U = G -> s and V = G' -> s
  v = linComb v e = Σᵘᵢ (vᵢ ◃ eᵢ)
  h v = h (Σᵘᵢ (vᵢ ◃ᵘ eᵢ))     -- LinTran(h,U,V)
      = Σᵛᵢ ( h (vᵢ ◃ᵘ eᵢ) )   -- LinTran(h,U,V)
      = Σᵛᵢ ( vᵢ ◃ᵛ h eᵢ)
  + Calculation
    h v
      = -- e is the canonical basis => v = Σᵁᵢ (scaleᵁ vᵢ eᵢ)
    h (Σᵁᵢ (scaleᵁ vᵢ eᵢ))
      = -- h distributes over Σ - note the change of type
    Σⱽᵢ (h (scaleᵁ vᵢ eᵢ))
      = -- h distributes over scale - note the change of type (again)
    Σⱽᵢ (scaleⱽ vᵢ (h eᵢ))
      = -- Def. of linComb
   linComb v (h ∘ e)
  + Note: to compute h v for any (infinitely many) v it is enough to
    store the results of h eᵢ for every i (finitely many). Each h eᵢ
    is a vector in V, thus we can store this collection in a table,
    usually called a *matrix*.
    file:Matrix_shape.png
      |     |      |     |     |
      |     |      |     |     |
      | h eₒ | h e₁ | ... | h eₙ |
      |     |      |     |     |
      |     |      |     |     |
  + This matrix is the "syntax" of a linear transformation and the
    linear function h : U -> V is the semantics.
  + evalMV m v = linComb v m
+ Example: der : P₃ -> P₂ as a linear transformation
  Def. Pₙ = { polynomials of degree ≤ n} = {0..n} -> REAL
    -- represented as coefficients
  evalₚ : Pₙ -> (REAL -> REAL)
  evalₚ a = Σᵢ scale aᵢ pᵢ
  Basis:
    p : {0..n} -> REAL -> REAL
    p i x = xⁱ
+ Example cont.: the matrix version DER of der : P₃ -> P₂
  + Step 1: type / dimensions of the target
    each column of the matrix represents a vector in the target space
    thus, here a polynomial in P₂
    represented by three coefficients: (think of a₀ + a₁*x + a₂*x²)
    thus we need three rows
  + Step 2: type / dimensions of the source
    there is one column for each basis vector in the source space
    thus, one for each of 1, x, x², x³
    thus we need four columns
  + file:DER3_shape.png
          | 0 | 1 | 0 | 0 |
    DER = | 0 | 0 | 2 | 0 |
          | 0 | 0 | 0 | 3 |
  + Step 3: fill in the resulting shape with the
    der (p i) = scale i (p (i-1))
          | 0 | 1 | 0 | 0 |
    DER = | 0 | 0 | 2 | 0 |
          | 0 | 0 | 0 | 3 |
+ Composing homomorphisms (here LinTran)
  + Typing: let A, B, C be vector spaces and hᵢ linear transformations
#+BEGIN_SRC text
	 h₂      h₁
     C <———— B <———— A
	    h₂∘h₁
     C <———————————— A
#+END_SRC
  + Property: "homomorphisms compose"
#+BEGIN_SRC text
     LinTran(h₁,   A,B) ∧
     LinTran(h₂,     B,C) ⇒
     LinTran(h₂∘h₁,A,  C)
#+END_SRC
+ Composing LinTran (towards matrix multiplication)
  + Typing + specification: let hᵢ = evalMV Mᵢ
#+BEGIN_SRC text
	 h₂      h₁
     C <———— B <———— A
         M₂      M₁

	    h₂∘h₁
     C <———————————— A
         mulM M₂ M₁
#+END_SRC
  + Property? (three variants)
     + ∃ mulM . evalMV (mulM M₂ M₁) = evalMV M₂ ∘ evalMV M₁
     + "can we compute a matrix for the composition h₂ ∘ h₁ from just M₂ and M₁?"
     + ∃ mulM . H2(evalMV, mulM, (∘))
+ Example:
#+BEGIN_SRC text
  A = a->REAL; B=b->REAL; C=c->REAL
  a={U,V}; b={0,1,2}; c={1}
       |      |      |      | 1 | 0 |
  M₁ = |h₁ eᵤ | h₁ eᵥ |  =   | 1 | 1 |
       |      |      |      | 0 | 1 |

  M₂ = | 0 | 2 | 0 |
  h₂ w = \i -> scale 2 (w i)   -- scaled projection

  mulM M₂ M₁ = | 2 | 2 |
#+END_SRC
+ Helper functions for vectors and matrices
  + Define some type synonyms:
#+BEGIN_SRC haskell
    type Vec s a = a -> s    -- a for "axes", s for "scalars"
    type Mat s a b = b -> Vec s a  =  b -> (a -> s)
#+END_SRC
  + And helper functions:
#+BEGIN_SRC haskell
    flip : (b -> a -> s) -> (a -> b -> s)
    flip op i j = op j i
    transpose : Mat s a b -> Mat s b a
    transpose m = \i j -> m j i
    getCol : Mat s a b -> a -> V s b
    getCol = flip
#+END_SRC
  + Also note a property of flip: (it is its own inverse)
#+BEGIN_SRC haskell
      ∀ m. flip (flip m) == m
#+END_SRC
    or, equivalently,
#+BEGIN_SRC haskell
      flip ∘ flip == id
#+END_SRC
+ How do we compute the matrix from a LinTran?
  Suppose we know
#+BEGIN_SRC haskell
    h : A -> B; LinTran(h,A,B)
#+END_SRC
  but we want to find an m such that
#+BEGIN_SRC haskell
    h = evalMV m
#+END_SRC
  Specification: the matrix should store the columns of h of the basis
#+BEGIN_SRC haskell
    getCol m i == h (e i)
  = -- Def. of (∘), simplification
    getCol m == h ∘ e
  = -- Def. getCol = flip
    flip m == h ∘ e
  = -- Apply flip to both sides
    flip (flip m) == flip (h ∘ e)
  = -- flip is its own inverse
    m == flip (h ∘ e)
#+END_SRC
  Thus, we can get from h to the corresponding matrix and back
#+BEGIN_SRC haskell
    m == flip (h ∘ e)
    h == evalMV m
#+END_SRC
  thus also the "round-trip property":
#+BEGIN_SRC haskell
    m == flip (evalMV m ∘ e)
#+END_SRC
  which can be seen as a specification of evalMV.
+ Compute the matrix multiplication (in a similar way):
  Start from the setting above:
#+BEGIN_SRC text
	 h₂      h₁
     C <———— B <———— A
         m₂      m₁

	    h₂∘h₁
     C <———————————— A
         mulM m₂ m₁
#+END_SRC
  where we know
#+BEGIN_SRC haskell
    h₁ = evalMV m₁;
    h₂ = evalMV m₂;
#+END_SRC

  Start computing (towards a definition of mulM):
#+BEGIN_SRC haskell
    getCol (mulM m₂ m₁) i
  = -- Specification of (mulM m₂ m₁)
    (h₂ ∘ h₁) eᵢ
  = -- Def. of (∘)
    h₂ (h₁ eᵢ)
  = -- Def. of h₂ and specification of m₁
    evalMV m₂ (getCol m₁ i)
  = -- Def. of (∘)
    (evalMV m₂ ∘ getCol m₁) i
#+END_SRC
  Thus we have
#+BEGIN_SRC haskell
    getCol (mulM m₂ m₁) == evalMV m₂ ∘ getCol m₁
#+END_SRC
  and we can apply flip to both sides (as before)
#+BEGIN_SRC haskell
    flip (getCol (mulM m₂ m₁)) == flip (evalMV m₂ ∘ getCol m₁)
#+END_SRC
  we notice  getCol = flip  and  flip ∘ flip = id
#+BEGIN_SRC haskell
    mulM m₂ m₁ == flip (evalMV m₂ ∘ getCol m₁)
#+END_SRC
  This is now a definition of mulM which satisfies its specification.
  (Reminder: evalMV m v = linComb v m = Σᵢ scale vᵢ mᵢ)
+ Summing up:
#+BEGIN_SRC haskell
  type A = Vec s a
  type B = Vec s b
  type C = Vec s c
  -- Notice that the b parameters makes sure the matrix dimensions match:
  mulM : Mat s a b -> Mat s b c -> Mat s a c
  mulM m₂ m₁ == flip (evalMV m₂ ∘ getCol m₁)

  evalMV = mulMV : Mat s a b -> Vec s a -> Vec s b
#+END_SRC
+ Perhaps some live-coding [[Live_7_1_2023.hs]]

** Lecture 7.2 / 8.1: Laplace Transforms
+ An application of linear algebra
+ ... and a method for solving ODEs
+ (Note: this transform is not implemented in Haskell in the course.)
*** [[file:DSLsofMath L7.2_L8.1 towards Laplace.pdf][L8.1a]]
+ Infinite-dimensional vector space
  + let V = {f : REAL -> REAL | f is smooth}
  + V is a vector space
  + Example vectors: exp, sin, cos, all polynomials, etc.
  + But not abs, discontinuous functions,
+ D (derivative) as a linear transform
  + D : V -> V
  + Example: D exp = exp
+ A family of exponentials (which will be used for Laplace later)
  + let gₛ t = exp (-s*t)
  + g is a family of smooth vectors
  + g : REAL -> V
  + D gₛ t = -s*exp (-s*t) = -s * g s t = scaleF (-s) gₛ t
  + D gₛ t = scaleF (-s) gₛ
  + file:gs_geogebra_with_def.png
+ Integral as a linear transform
  + I : V -> V
  + I f x = integral 0 x f
+ Some properties of D and I
  + D (I f) = D F = f
  + I (D F) x = I f x = F x - F 0
  + D (f*g) = D f * g  +  f * D g
  + I (D (F*G)) x = (F*G) x - (F*G) 0 = F x * G x  -  F 0 * G 0

+ "discovering" the Laplace transform
*** [[file:../08/DSLsofMath L8.2a start.pdf][L8.1b]]
+ Laplace transform examples: exp, sin, cos
+ Laplace for solving f''+2f=3f', f 0 = 0, f' 0 = 1
+ Laplace summary
* Note
+ Nice examples of vectors, matrices, including Show instances:
  [[file:../../old/2021/L/07/Live_7_2.lhs]]
